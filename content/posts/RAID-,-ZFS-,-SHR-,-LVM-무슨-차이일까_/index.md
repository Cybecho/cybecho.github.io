---
title: "RAID , ZFS , SHR , LVM 무슨 차이일까?"
date: 2026-01-20T04:09:00.000Z
draft: false
tags: ["ProxMox", "NAS"]
series: ["Let's Homelab!", "Infra & Network"]
description: "RAID, LVM, ZFS, SHR는 데이터 관리 방식에서 차이를 보이며, 각각 성능과 유연성에 영향을 미친다. RAID는 하드웨어 추상화에 집중하고, LVM은 물리 장치와 파일 시스템 간의 중간 계층을 제공한다. ZFS는 파일 시스템과 RAID 기능을 통합하여 높은 데이터 일관성을 제공하며, SHR은 RAID와 LVM을 결합하여 다양한 용량의 디스크를 효율적으로 활용한다. 각 기술의 메타데이터 관리 방식은 복구 능력에 영향을 미치며, ZFS는 Copy-on-Write 방식을 통해 강력한 복구 기능을 제공한다."
notion_id: "2ee1bab9-e3f8-806d-b504-fc9206958384"
notion_url: "https://www.notion.so/RAID-ZFS-SHR-LVM-2ee1bab9e3f8806db504fc9206958384"
---

# RAID , ZFS , SHR , LVM 무슨 차이일까?

> **Summary**
> RAID, LVM, ZFS, SHR는 데이터 관리 방식에서 차이를 보이며, 각각 성능과 유연성에 영향을 미친다. RAID는 하드웨어 추상화에 집중하고, LVM은 물리 장치와 파일 시스템 간의 중간 계층을 제공한다. ZFS는 파일 시스템과 RAID 기능을 통합하여 높은 데이터 일관성을 제공하며, SHR은 RAID와 LVM을 결합하여 다양한 용량의 디스크를 효율적으로 활용한다. 각 기술의 메타데이터 관리 방식은 복구 능력에 영향을 미치며, ZFS는 Copy-on-Write 방식을 통해 강력한 복구 기능을 제공한다.

---

![Image](image_a3ca02cc1aaa.png)


[https://www.perplexity.ai/search/raid-zfs-lvm-shr-edaehae-gagga-hyGdcu6jQCWJLIkbpyumMw#0](https://www.perplexity.ai/search/raid-zfs-lvm-shr-edaehae-gagga-hyGdcu6jQCWJLIkbpyumMw#0)

## [주제 1: 시스템 계층에 따른 저장 장치 추상화 모델 비교]

RAID, LVM, ZFS, SHR은 데이터를 관리하는 추상화 계층이 각기 다르며, 이는 성능과 유연성에 직접적인 영향을 미칩니다.
RAID는 가장 하위 계층인 블록 디바이스 드라이버 레벨에서 작동합니다. 리눅스 커널의 md 모듈을 통해 물리 디스크의 블록들을 스트라이핑하거나 패리티를 생성하여 OS에 단일 블록 장치를 제공하는 것이 핵심입니다. 이는 순수한 하드웨어 추상화에 집중하므로 스냅샷이나 동적 확장과 같은 상위 수준의 관리 기능은 배제됩니다.
LVM은 물리 장치와 파일시스템 사이의 중간 매핑 계층으로 존재합니다. 커널의 dm 모듈을 이용해 논리 주소(LE)를 물리 주소(PE)로 변환하는 매핑 테이블을 관리하며, 이를 통해 여러 물리 디스크를 하나의 볼륨 그룹으로 묶고 용량을 유연하게 할당합니다. 하지만 매 I/O마다 테이블을 조회해야 하는 오버헤드가 발생하며, 데이터 보호를 위해 하위 RAID 계층에 의존하는 구조를 가집니다.
ZFS는 파일시스템, 볼륨 매니저, RAID 기능을 단일 스택으로 통합한 설계 철학을 따릅니다. 별도의 매핑 테이블 없이 DMU(데이터 관리 유닛)와 SPA(스토리지 풀 할당자)가 유기적으로 작동하여 논리 블록을 가상 주소 풀 내의 물리 위치로 할당합니다.
마지막으로 SHR은 리눅스의 mdadm과 LVM을 시놀로지가 자동화하여 결합한 형태입니다. 하위에서 RAID 5/6를 구성하고 그 위에서 LVM으로 용량을 관리함으로써, 서로 다른 용량의 디스크를 효율적으로 활용할 수 없는 전통적인 RAID의 한계를 극복합니다.

## [주제 2: 메타데이터 관리 구조 및 데이터 일관성 유지 모델]

각 시스템이 스토리지 정보를 기록하는 메타데이터 관리 방식은 시스템 장애 시 복구 능력과 직결됩니다.
RAID는 디스크의 특정 섹터에 '슈퍼블록'을 저장하며, 데이터 변경 시 해당 위치를 덮어쓰는(In-place) 방식을 사용합니다. 이 구조는 전원 차단 시 데이터와 패리티의 불일치가 발생하는 'Write Hole' 현상에 취약하다는 태생적 한계가 있습니다.
LVM은 물리 볼륨의 시작과 끝에 레이블 헤더와 메타데이터 영역(MDA)을 다중으로 배치합니다. 텍스트 형식의 메타데이터를 여러 곳에 중복 저장하여 손상 시 복구 가능성을 높이지만, 일관성 모델 자체는 파일시스템의 능력에 의존적입니다.
반면 ZFS는 'Uberblock Array'와 트랜잭션 그룹(TXG) 모델을 통해 고도의 일관성을 제공합니다. 데이터를 절대 덮어쓰지 않는 Copy-on-Write(또는 Redirect-on-Write) 방식을 채택하며, 128개의 Uberblock 슬롯을 순환하며 기록합니다. 이는 최대 약 1시간 이전의 상태로 메타데이터를 되돌릴 수 있는 강력한 복구 기능을 제공하며, 'Write Hole' 문제를 근본적으로 차단합니다.

## [주제 3: 패리티 계산 및 주소 변환의 논리적 메커니즘]

데이터 보호를 위한 수학적 연산과 주소 변환 방식에서도 뚜렷한 차이가 확인됩니다.
RAID 5와 6는 주로 배타적 논리합인 XOR 연산을 기반으로 패리티를 계산합니다.
A \oplus B = P의 성질을 이용하여 단일 디스크 손실 시 B = A \oplus P와 같이 데이터를 복원합니다. 이는 계산이 매우 빠르지만 소규모 쓰기 시 전체 스트라이프를 읽어야 하는 쓰기 페널티가 발생합니다.
ZFS의 RAIDZ는 더 복잡한 유한체(Galois Field) 연산을 사용합니다. RAIDZ1에서 RAIDZ3까지 최대 3개의 패리티를 생성할 수 있으며, 이는 단순 XOR로는 불가능한 3중 디스크 손실 복구를 가능하게 합니다. CPU 부하가 높지만 최신 벡터화 최적화(AVX)를 통해 효율성을 확보했습니다.
주소 변환에 있어서 LVM은 명시적인 LE-PE 매핑 테이블을 커널 메모리에 유지하며 I/O를 처리합니다. 반면 ZFS는 할당 시점에 라운드 로빈 방식으로 vdev 간 동적 스트라이핑을 수행하므로, 별도의 주소 변환 오버헤드 없이 풀 확장과 동시에 성능 최적화가 이루어지는 구조를 가집니다.

## [주제 4: 커널 구현 및 실무 적용 비교 분석]

각 기술의 커널 구현체와 권장 시나리오는 다음과 같이 요약됩니다.

| 기술 | 커널 모듈 | 메타데이터 업데이트 | 주요 장점 |
| --- | --- | --- | --- |
| RAID | md | In-place (덮어쓰기) | 최소한의 오버헤드, 범용성 |
| LVM | dm | 텍스트 기반 다중 저장 | 유연한 용량 할당, 온라인 이동 |
| ZFS | zfs | Copy-on-Write (순환) | 데이터 무결성 검증, 스냅샷 효율성 |
| SHR | md + lvm | 혼합 방식 | 이종 용량 디스크 최적 |

![Image](image_c0d933d27fc2.png)

![Image](image_80cd0627325b.png)

![Image](image_17fa3145bb22.png)

![Image](image_95114a412706.png)


단순히 블록 레벨의 보호가 우선인 환경에서는 하드웨어 RAID나 mdadm이 적합하며, 유연한 파티션 관리가 필요한 리눅스 서버 환경에서는 LVM이 권장됩니다. 데이터의 절대적인 무결성과 무제한에 가까운 스냅샷 기능이 필수적인 엔터프라이즈 스토리지에는 ZFS가 최적의 대안입니다. 개인용 NAS와 같이 디스크 용량이 제각각인 환경에서는 SHR이 가장 높은 공간 효율성을 제공합니다.

